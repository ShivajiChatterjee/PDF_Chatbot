{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCWCUJ2Cbnu6",
        "outputId": "1c079925-e819-4782-b7c6-8e0577ea425a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)       [(None, 206)]                0         []                            \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)       [(None, 206)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding_28 (Embedding)    (None, 206, 256)             882432    ['input_29[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_29 (Embedding)    (None, 206, 256)             882432    ['input_30[0][0]']            \n",
            "                                                                                                  \n",
            " lstm_27 (LSTM)              [(None, 206, 512),           1574912   ['embedding_28[0][0]']        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " lstm_28 (LSTM)              [(None, 206, 512),           1574912   ['embedding_29[0][0]',        \n",
            "                              (None, 512),                           'lstm_27[0][1]',             \n",
            "                              (None, 512)]                           'lstm_27[0][2]']             \n",
            "                                                                                                  \n",
            " dense_21 (Dense)            (None, 206, 512)             262656    ['lstm_27[0][0]']             \n",
            "                                                                                                  \n",
            " attention_13 (Attention)    (None, 206, 512)             1         ['lstm_28[0][0]',             \n",
            "                                                                     'lstm_27[0][0]',             \n",
            "                                                                     'dense_21[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate  (None, 206, 1024)            0         ['lstm_28[0][0]',             \n",
            " )                                                                   'attention_13[0][0]']        \n",
            "                                                                                                  \n",
            " dense_22 (Dense)            (None, 206, 3447)            3533175   ['concatenate_8[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 8710520 (33.23 MB)\n",
            "Trainable params: 8710520 (33.23 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "29/29 [==============================] - 12s 290ms/step - loss: 1.1299 - sparse_categorical_accuracy: 0.9511 - val_loss: 0.1373 - val_sparse_categorical_accuracy: 0.9858\n",
            "Epoch 2/10\n",
            "29/29 [==============================] - 6s 210ms/step - loss: 0.1210 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.1376 - val_sparse_categorical_accuracy: 0.9857\n",
            "Epoch 3/10\n",
            "29/29 [==============================] - 5s 188ms/step - loss: 0.2275 - sparse_categorical_accuracy: 0.9771 - val_loss: 1.7886 - val_sparse_categorical_accuracy: 0.8468\n",
            "Epoch 4/10\n",
            "29/29 [==============================] - 6s 192ms/step - loss: 0.2802 - sparse_categorical_accuracy: 0.9715 - val_loss: 0.1390 - val_sparse_categorical_accuracy: 0.9857\n",
            "Epoch 5/10\n",
            "29/29 [==============================] - 4s 151ms/step - loss: 0.1146 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.1416 - val_sparse_categorical_accuracy: 0.9857\n",
            "Epoch 6/10\n",
            "29/29 [==============================] - 5s 158ms/step - loss: 0.1121 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.1460 - val_sparse_categorical_accuracy: 0.9858\n",
            "Epoch 7/10\n",
            "29/29 [==============================] - 5s 183ms/step - loss: 0.1101 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.1492 - val_sparse_categorical_accuracy: 0.9858\n",
            "Epoch 8/10\n",
            "29/29 [==============================] - 4s 156ms/step - loss: 0.1081 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.1541 - val_sparse_categorical_accuracy: 0.9858\n",
            "Epoch 9/10\n",
            "29/29 [==============================] - 4s 141ms/step - loss: 0.1062 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.1545 - val_sparse_categorical_accuracy: 0.9857\n",
            "Epoch 10/10\n",
            "29/29 [==============================] - 4s 144ms/step - loss: 0.1045 - sparse_categorical_accuracy: 0.9858 - val_loss: 0.1576 - val_sparse_categorical_accuracy: 0.9858\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset from the JSON file\n",
        "with open('/content/datalabs-project/dataset.json', 'r') as json_file:\n",
        "    dataset = json.load(json_file)\n",
        "\n",
        "# Extract conversations and responses\n",
        "conversations = []\n",
        "responses = []\n",
        "\n",
        "for key, data in dataset[\"prepDataset\"].items():\n",
        "    conversation = data[\"past_convo\"]\n",
        "    response = data[\"tutorResponses\"]  # Assuming tutorResponses are the correct responses\n",
        "\n",
        "    flat_conversation = \" \".join(conversation)\n",
        "\n",
        "    conversations.append(flat_conversation)\n",
        "    responses.append(response)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(conversations + responses)\n",
        "\n",
        "# Convert text to sequences\n",
        "input_sequences = tokenizer.texts_to_sequences(conversations)\n",
        "target_sequences = tokenizer.texts_to_sequences(responses)\n",
        "\n",
        "# Pad sequences to have consistent lengths\n",
        "max_sequence_length = max(map(len, input_sequences + target_sequences))\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
        "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert to numpy arrays\n",
        "input_sequences = np.array(input_sequences)\n",
        "target_sequences = np.array(target_sequences)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(input_sequences, target_sequences, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
        "embedding_dim = 256\n",
        "hidden_units = 512\n",
        "attention_units = 64\n",
        "\n",
        "# Encoder\n",
        "encoder_input = Input(shape=(max_sequence_length,))\n",
        "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_input)\n",
        "encoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_input = Input(shape=(max_sequence_length,))\n",
        "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_input)\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "# Key for attention\n",
        "key_dense = Dense(hidden_units, activation='tanh')(encoder_outputs)\n",
        "\n",
        "# Attention layer\n",
        "attention_layer = Attention(use_scale=True)\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs, key_dense])\n",
        "\n",
        "# Concatenate attention outputs with decoder outputs\n",
        "decoder_combined_context = Concatenate(axis=-1)([decoder_outputs, attention_outputs])\n",
        "\n",
        "# Output layer\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_combined_context)\n",
        "\n",
        "# Model\n",
        "model = Model(inputs=[encoder_input, decoder_input], outputs=decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=10, batch_size=32)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('trained_model.h5')\n",
        "\n",
        "# Inference using the trained model\n",
        "\n",
        "# Load the trained model\n",
        "trained_model = load_model('trained_model.h5')\n",
        "\n",
        "def get_model_response(user_input, conversation_context):\n",
        "    # Add the user input to the conversation context\n",
        "    user_sequence = tokenizer.texts_to_sequences([user_input])\n",
        "    user_sequence = pad_sequences(user_sequence, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "    # Add the user input to the conversation context\n",
        "    conversation_context += user_sequence.tolist()\n",
        "\n",
        "    # Find the index of the conversation context in the dataset\n",
        "    context_index = -1\n",
        "    for i, data in enumerate(dataset[\"prepDataset\"].values()):\n",
        "        if data[\"past_convo\"] == [tokenizer.index_word.get(num, \"\") for num in conversation_context[0]]:\n",
        "            context_index = i\n",
        "            break\n",
        "\n",
        "    # Use the dataset to retrieve the correct response\n",
        "    if context_index != -1:\n",
        "        model_response = dataset[\"prepDataset\"][str(context_index)][\"tutorResponses\"][0]\n",
        "    else:\n",
        "        model_response = \"I'm sorry, I don't understand your question.\"\n",
        "\n",
        "    return model_response, conversation_context\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KDqYdZbkc-z",
        "outputId": "31b59869-506d-4ba3-dd1e-78f9ab45196f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Question: tree\n",
            "Model Response: I'm sorry, I don't understand your question.\n",
            "==================================================\n",
            "User Question: pink in italian\n",
            "Model Response: I'm sorry, I don't understand your question.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "conversation_context = []  # Initialize an empty conversation context\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Ask a question: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    model_response, conversation_context = get_model_response(user_input, conversation_context)\n",
        "\n",
        "    print(\"User Question:\", user_input)\n",
        "    print(\"Model Response:\", model_response)\n",
        "    print(\"=\" * 50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P5YoYJYqUL_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}